---
title: "Homework 5"
author: "James Dalgleish"
date: "November 2, 2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
#setMKLthreads(4)
```

### Problem 1

To begin, we'll download the file, extract it's contents, and clean up the unnecessary files. 
```{r data_dl}
#Download and extract source data from zip archive.
download.file(url = "http://p8105.com/data/hw5_data.zip", 
              destfile =  "hw5_data.zip")
unzip(zipfile = "hw5_data.zip", exdir = "data", junkpaths = T) #extract the files into this new directory
study_csvs <- list.files(path = "data",pattern = "^.[A-z]*.*\\.csv$", full.names = T) #grab just the real CSVs beginning with a letter and ending with a .csv, avoiding unimportant files in the archive.

#delete the extra csvs and ._data in the __MACOSX folder of the archive.
#a setdiff will include the . and .., which we can't remove, so specific commands are needed.
#I am unaware of how the archive will look in MACOS (I assume simpler), but just in case, I'll make this cleanup windows-specific.
if ( Sys.info()['sysname'] == "Windows") { 
extra_files <- list.files('data',all.files = T, "^[.].*\\.csv$", full.names = T)
#grabs the files that are csv, but begin with a period (not the real datafiles).
file.remove(extra_files) #deletes these extra csvs.
file.remove("./data/._data") #deletes this extra ._data file.
file.remove("hw5_data.zip") #deletes the zip file
}
```
Now that we have the raw data, we'll import it and convert the contents of the individual subject data into a comprehensive table in long format that is amenable to data analysis, incorporating information about the subject contained in the filename.
```{r data_import, message = F}
read_w_fn <- function(csv, basename = T) { #the basename argument ensures the function is more generalizable to future problems where the full file path is desired in the output (in which case the user can set this flag to false).
  if (basename) {
  df <- read_csv(csv) %>% #Read in csv (csv filename specified by the string).
    mutate(source_file = #add a column for filename from whence the rows came.
             basename(csv) #Note: Putting in a single value will always work to create a column by recycling.
           #see dplyr documentation.
              #https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html
           #basename strips the directory information from the filename.
                 )
  } else {
    df <- read_csv(csv) %>% 
    mutate(source_file = csv) #This option just takes the filename as is, without removing the path from the full filename.
  }
}

study_data <- study_csvs   %>%  #Creates a dataframe from the columns.
   map_df(.f = read_w_fn) %>% #reads in all the csvs and puts them together into a single dataframe by binding the rows together. 
  gather(key = "week", value = "measurement", starts_with("week")) %>%  #converts weeks to long format.
  mutate(week = (str_replace(week,"week_","") %>% 
           as.numeric()), #creates a numeric week variable
         arm = str_replace(basename(source_file),"_.*$",""), #creates a categorical arm variable by removing the text after con or exp.
         
         subject_id = (str_extract( #extracts the subject number from the filename.
           basename(source_file), #removes the directory information from the filename.
          pattern =  "(\\d)+") %>% #specifies the numbers within the filename as the string to be extracted.
            str_c(arm) %>%  #adding this to the subject stops false connections that can alterate between control and treatment in geom_line.
                         as.factor) #converts subject ID to numeric
         ) %>% 
  arrange(arm,week,subject_id) #sorts by treatment arm, week, and subject_id.

```

With the data in long tibble format, we can then plot the study measurements over time with a line for each subject, colored by the treatment arm. The study measurements are measured vertically with time across the x axis.
```{r spaghetti_plot}
spaghetti_con_exp <- ggplot(data = study_data,  
                            aes(x = week, y = measurement, 
                                color = arm, group = subject_id)) + #Sets mappings displayed above in the paragraph description.
  geom_line() + #Specifies the plot type to line.
  labs(title = "Subject Study Measurements by Week", fill = "Study Arm",
       x = "Week", y = "Measurement") #Sets appropriate labels. 
spaghetti_con_exp #plots the output

```
On the whole, it appears that the patients in the control group are generally lower in measurement than those in the experimental group. Subject 10 is anomalously high in the control condition.

### Problem 2

Now, we'll grab homicide data for analysis.
```{r}
homicide_data <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% #read in data.
  mutate( victim_first = iconv(x = victim_first, from = "latin1", to =  "ASCII",sub = ""),
          victim_last = iconv(x = victim_last, from = "latin1", to =  "ASCII",sub = ""),
          #This removes nonstandard characters that cause problems with skimr.
          city_state = str_c(city,", ",state), #create city state variable by string concatenation.
          unresolved = disposition %in% c("Closed without arrest","Open/No arrest"),
          #creates a variable for whether the case was  unresolved.
          resolved = disposition %in% c("Closed by arrest")) 
#creates a variable for whether the case was resolved, to double check the numbers.
#Included the resolved cases as a separate variable to clearly show that the resolved and unresolved cases sum to the total (and hence the calculations make sense).
hom_summ_tbl <- homicide_data %>% 
  group_by(city_state) %>%  
  summarise(total = n(), #get the total number of cases by city_state combination.
            unresolved_cases = sum(unresolved), 
        #Gets the unresolved cases of each city state combination.
            resolved_cases = sum(resolved)
        #Gets the resolved cases of each city state combination.
            ) %>% 
  select(city_state, unresolved_cases, resolved_cases, total) 
  #rearranges the columns

 hom_summ_tbl %>% 
  knitr::kable(col.names = c("City, State", "Unresolved Cases", "Resolved Cases", "Total Cases"), format = "markdown", padding = 10) #Displays the table in a more presentable format.
#The numbers add up: unresolved + resolved = total.
 library(magrittr)
 balt_res <- hom_summ_tbl %>%
   filter(city_state == "Baltimore, MD") %>%  #filter for only baltimore, MD.
   select(unresolved_cases, total) %>%  #grab the unresolved cases and total rows
   rowwise() %$% #Make the next function work along rows (a single row in this case)
   #The magrittr dollar sign pipe (part of tidyverse, see documentation) allows exposition of variables, which allows functions without a data argument, like prop.test, to work. Essentially, without this line, prop.test won't be able to understand where the unresolved cases are.
   prop.test(x = unresolved_cases, n = total) #prop.test grabs the unresolved cases and total and estimates a proportion utilizing the yates continuity correction by default and producing a 95% confidence interval.
 balt_res %>% 
   broom::tidy() %>% #converts the LM results to a tibble
   knitr::kable() #display table using kable
 balt_ci_est <- balt_res %>% 
   broom::tidy() %>% 
   select(estimate, conf.low, conf.high) #Grabs estimate and CI.


  est_ci_cityst <- hom_summ_tbl  %>% #Grabs homicide summary table (created earlier)
     mutate(ci_est =
              map2(.x = unresolved_cases, .y = total,
        .f = ~(prop.test(x = .x, n = .y) %>% broom::tidy()) #broom::tidy reformats the output.
     )#calculates proportion and CI for every city in the dataset
     ) %>% unnest() %>% #undoes the grouping of the lists within each row, converting listcols to columns.
     select(city_state, estimate, conf.low, conf.high) #selects the city_state as a label, the estimate and CI for all cities and stores it as est_ci_cityst.
  est_ci_cityst %>% 
 mutate(city_state = forcats::fct_reorder(.f = 
                           city_state %>% as.factor(),
                         estimate) #reorders the city labels in the dataframe by converting to a factor and ordering it by the estimate value, in ascending order. 
 ) %>% 
 ggplot( 
       aes(x = city_state, #creates a plot with city along the x axis.
           y = estimate, ymin = conf.low, ymax = conf.high) #estimate along the y axis, with the errorbar min and max being set by the confidence interval limits
       )  +
  geom_point() + #adds a point for the estimate for each city
  geom_errorbar() +  #adds an error bar for the confidence interval for each city.
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + #rotates the labels so they don't overlap.
    labs(title = "Estimated Proportion of Unresolved Homicides by City, State",
         y = "Estimated proportion", x = "City, State") #Adds appropriate labels.
```
Above is a plot showing the estimates of unresolved homicides among all cases within the cities. Noticably, large cities like chicago and Baltimore have a lot of cases that go unsolved.  New Orleans is not surprising either. San Bernadino as the exception and may suggest an insufficient police force to solve the homicides, but additional data would be needed to confirm this.
Tulsa, Alabama is a nonexistent city as per the [US census.]:https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk and represents an error in the dataset. The geographic coordinates suggest the homicide may have occurred in Tulsa, Oklaholma instead. At this point, one could report this to the collaborator and either correct it to Tulsa, OK, or remove it with a filter statement, depending on discussions.